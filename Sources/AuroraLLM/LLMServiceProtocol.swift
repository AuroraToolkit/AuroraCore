//
//  LLMServiceProtocol.swift
//  Aurora
//
//  Created by Dan Murrell Jr on 8/19/24.
//

import Foundation

// MARK: - TokenAdjustmentPolicy

/**
 The `TokenAdjustmentPolicy` enum specifies how input and output token limits are handled for an LLM service.

 This policy provides flexibility in managing requests when they exceed the token constraints of a specific service, ensuring developers can configure behavior that aligns with their use case.

 - Note: Each service can have separate policies for input and output tokens, allowing fine-grained control over request handling.
 */
public enum TokenAdjustmentPolicy {
    /**
     Automatically adjusts the token limits to match the service's constraints.

     - For input tokens: The request's content is trimmed to fit within the service's input token limit using the specified trimming strategy.
     - For output tokens: The request's `maxTokens` is reduced to the service's `maxOutputTokens`.

     - Use Case: This policy is ideal when ensuring compatibility with the service is more important than strict adherence to the original request's configuration.
     */
    case adjustToServiceLimits

    /**
     Strictly enforces the token limits defined in the request.

     - For input tokens: If the request exceeds the service's input token limit, the request will fail.
     - For output tokens: If the request's `maxTokens` exceeds the service's `maxOutputTokens`, the request will fail.

     - Use Case: This policy is ideal for debugging or when the integrity of the original request must be preserved without any modifications.
     */
    case strictRequestLimits
}

// MARK: - LLMServiceProtocol

/**
 The `LLMServiceProtocol` defines the interface for interacting with different LLM (Language Learning Model) services.

 Conforming types (e.g., `OpenAIService`, `AnthropicService`, `OllamaService`) must implement this protocol to enable communication with their respective LLM backends.

 This protocol ensures that all LLM services handle requests and responses consistently, allowing the client to interact with multiple LLMs in a unified way.

 - Note: Each service can define its own token limits and handle authentication in its unique way.
 */
public protocol LLMServiceProtocol {
    /**
     The name of the LLM service vendor (e.g., "OpenAI", "Anthropic", "Ollama").
     */
    var vendor: String { get }

    /**
     The name of the service instance, which can be customized during initialization
     */
    var name: String { get set }

    /**
     The maximum context window size (total tokens, input + output) supported by the service.

     - Note: Requests should not exceed this limit to ensure they are processed correctly.
     */
    var contextWindowSize: Int { get }

    /**
     The maximum number of tokens allowed for output (completion) in a single request.

     - Important: This value may differ between services. For example, OpenAI may have a higher token limit compared to other LLMs.
     This value must be less than or equal to `contextWindowSize` to ensure the request fits within the service's capacity.
     - Note: Many services have a max output token limit lower than the context window size.
     */
    var maxOutputTokens: Int { get }

    /**
     Specifies the policy to handle input tokens when they exceed the service's input token limit.

     - SeeAlso: `TokenAdjustmentPolicy`
     */
    var inputTokenPolicy: TokenAdjustmentPolicy { get set }

    /**
     Specifies the policy to handle output tokens when they exceed the service's max output token limit.

     - SeeAlso: `TokenAdjustmentPolicy`
     */
    var outputTokenPolicy: TokenAdjustmentPolicy { get set }

    /// The default system prompt for this service, used to set the behavior or persona of the model.
    var systemPrompt: String? { get set }

    /**
     Sends a request to the LLM service asynchronously and returns the response.

     - Parameter request: The `LLMRequest` object containing the prompt and configuration for the LLM.
     
     - Returns: An `LLMResponseProtocol` containing the text generated by the LLM.
     - Throws: An error if the request fails due to network issues, invalid parameters, or API errors.
     */
    func sendRequest(_ request: LLMRequest) async throws -> LLMResponseProtocol

    /**
     Sends a request to the LLM service asynchronously with support for streaming.

     - Parameters:
        -  request: The `LLMRequest` object containing the prompt and configuration for the LLM.
        -  onPartialResponse: A closure that handles partial responses during streaming.

     - Returns: An `LLMResponseProtocol` containing the final text generated by the LLM.
     - Throws: An error if the request fails due to network issues, invalid parameters, or API errors.
     */
    func sendStreamingRequest(_ request: LLMRequest, onPartialResponse: ((String) -> Void)?) async throws -> LLMResponseProtocol
}
