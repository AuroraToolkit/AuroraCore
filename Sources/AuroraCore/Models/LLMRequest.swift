//
//  LLMRequest.swift
//  Aurora
//
//  Created by Dan Murrell Jr on 8/19/24.
//

import Foundation

/**
 A struct representing the input to be sent to the Language Learning Model (LLM).
 This structure allows customization of various parameters to influence the response generated by the LLM.
 */
public struct LLMRequest {
    /// The text prompt to be processed by the LLM.
    public let prompt: String

    /// The sampling temperature to control randomness (values between 0.0 to 1.0). Higher values (e.g., 1.0) make the output more random, while lower values (e.g., 0.0) make it more deterministic.
    public let temperature: Double

    /// The maximum number of tokens in the generated response.
    public let maxTokens: Int

    /// Nucleus sampling parameter that limits sampling to the top percentile of tokens. Lower values narrow the scope of the sampling to the most likely tokens.
    public let topP: Double

    /// A penalty applied to reduce the repetition of the same tokens in the response.
    public let frequencyPenalty: Double

    /// A penalty applied to encourage the introduction of new tokens into the response, promoting variety.
    public let presencePenalty: Double

    /// Sequences of tokens that signal the LLM to stop generating further tokens when encountered in the response.
    public let stopSequences: [String]?

    /// The specific LLM model to use for processing (e.g., "gpt-3.5-turbo", "davinci"). If not specified, the default model for the service will be used.
    public let model: String?

    /// A map of token biases, allowing customization of the likelihood of specific tokens appearing in the response.
    public let logitBias: [String: Double]?

    /// An optional user identifier, which can be used for tracking, moderation, or specific user-based adjustments.
    public let user: String?

    /**
     Initializes a new `LLMRequest` with customizable parameters and reasonable defaults for most fields.

     - Parameters:
        - prompt: The input text to be processed by the LLM.
        - temperature: A value between 0.0 and 1.0 controlling the randomness of the response (default is 0.7).
        - maxTokens: The maximum number of tokens to generate in the response (default is 256).
        - topP: The top probability value used for nucleus sampling (default is 1.0, meaning no nucleus sampling is applied).
        - frequencyPenalty: A penalty to discourage token repetition in the response (default is 0.0).
        - presencePenalty: A penalty to encourage the introduction of new tokens in the response (default is 0.0).
        - stopSequences: An optional array of strings that will stop the response generation when encountered.
        - model: An optional string representing the model to use (default is nil, meaning the default model for the service will be used).
        - logitBias: An optional dictionary that maps tokens to biases, allowing adjustment of token probabilities (default is nil).
        - user: An optional string representing a user identifier for tracking purposes (default is nil).
     */
    public init(prompt: String,
         temperature: Double = 0.7,
         maxTokens: Int = 256,
         topP: Double = 1.0,
         frequencyPenalty: Double = 0.0,
         presencePenalty: Double = 0.0,
         stopSequences: [String]? = nil,
         model: String? = nil,
         logitBias: [String: Double]? = nil,
         user: String? = nil) {

        self.prompt = prompt
        self.temperature = temperature
        self.maxTokens = maxTokens
        self.topP = topP
        self.frequencyPenalty = frequencyPenalty
        self.presencePenalty = presencePenalty
        self.stopSequences = stopSequences
        self.model = model
        self.logitBias = logitBias
        self.user = user
    }
}
