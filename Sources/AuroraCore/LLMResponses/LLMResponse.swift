//
//  LLMResponse.swift
//  Aurora
//
//  Created by Dan Murrell Jr on 8/19/24.
//

import Foundation

/**
 A protocol that defines a common interface for the response of different LLM services.

 Conforming types must implement:
 - `text`: The main textual response generated by the LLM.
 - `model`: The model used to generate the response, if available.
 - `tokenUsage`: Optional token usage data, providing information about token consumption during the request.

 This protocol allows for consistent interaction with different LLM services while accommodating their unique response formats.
 */
public protocol LLMResponseProtocol {
    /// The generated text from the LLM.
    var text: String { get }

    /// The model used to generate the text, if available.
    var model: String? { get }

    /// Token usage information, if available.
    var tokenUsage: LLMTokenUsage? { get }
}

/**
 A structure representing token usage information.
 This is used to track the number of tokens used for prompts, completions, and the total request.
 */
public struct LLMTokenUsage {
    /// The number of tokens used in the prompt.
    public let promptTokens: Int

    /// The number of tokens used in the completion.
    public let completionTokens: Int

    /// The total number of tokens used in the request.
    public let totalTokens: Int

    /// Initializes the `LLMTokenUsage` with the given token counts.
    public init(promptTokens: Int, completionTokens: Int, totalTokens: Int) {
        self.promptTokens = promptTokens
        self.completionTokens = completionTokens
        self.totalTokens = totalTokens
    }
}
